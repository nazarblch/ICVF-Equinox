{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import jax\n",
    "import os\n",
    "os.environ[\"D4RL_SUPPRESS_IMPORT_ERROR\"] = \"1\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "import jax\n",
    "import equinox.nn as nn\n",
    "import functools\n",
    "\n",
    "hidden_dims = [256, 256]\n",
    "state_dim=29\n",
    "rng = jax.random.PRNGKey(42)\n",
    "network_cls = functools.partial(nn.MLP, in_size=state_dim, out_size=hidden_dims[-1],\n",
    "                                        width_size=hidden_dims[0], depth=len(hidden_dims),\n",
    "                                        final_activation=jax.nn.relu)\n",
    "phi_net = network_cls(key=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_phi_net=eqx.tree_deserialise_leaves(\"../icvf_model.eqx\", phi_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  layers=(\n",
       "    Linear(\n",
       "      weight=f32[256,29],\n",
       "      bias=f32[256],\n",
       "      in_features=29,\n",
       "      out_features=256,\n",
       "      use_bias=True\n",
       "    ),\n",
       "    Linear(\n",
       "      weight=f32[256,256],\n",
       "      bias=f32[256],\n",
       "      in_features=256,\n",
       "      out_features=256,\n",
       "      use_bias=True\n",
       "    ),\n",
       "    Linear(\n",
       "      weight=f32[256,256],\n",
       "      bias=f32[256],\n",
       "      in_features=256,\n",
       "      out_features=256,\n",
       "      use_bias=True\n",
       "    )\n",
       "  ),\n",
       "  activation=<wrapped function relu>,\n",
       "  final_activation=<wrapped function relu>,\n",
       "  use_bias=True,\n",
       "  use_final_bias=True,\n",
       "  in_size=29,\n",
       "  out_size=256,\n",
       "  width_size=256,\n",
       "  depth=2\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_phi_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_linear = lambda x: isinstance(x, eqx.nn.Linear)\n",
    "get_weights = lambda m: [x.weight\n",
    "                         for x in jax.tree_util.tree_leaves(m, is_leaf=is_linear)\n",
    "                         if is_linear(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_weights(new_phi_net)[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_partial = eqx.tree_at(lambda mlp: mlp.layers[-1], new_phi_net, phi_net.layers[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  layers=(\n",
       "    Linear(\n",
       "      weight=f32[256,29],\n",
       "      bias=f32[256],\n",
       "      in_features=29,\n",
       "      out_features=256,\n",
       "      use_bias=True\n",
       "    ),\n",
       "    Linear(\n",
       "      weight=f32[256,256],\n",
       "      bias=f32[256],\n",
       "      in_features=256,\n",
       "      out_features=256,\n",
       "      use_bias=True\n",
       "    ),\n",
       "    Linear(\n",
       "      weight=f32[256,256],\n",
       "      bias=f32[256],\n",
       "      in_features=256,\n",
       "      out_features=256,\n",
       "      use_bias=True\n",
       "    )\n",
       "  ),\n",
       "  activation=<wrapped function relu>,\n",
       "  final_activation=<wrapped function relu>,\n",
       "  use_bias=True,\n",
       "  use_final_bias=True,\n",
       "  in_size=29,\n",
       "  out_size=256,\n",
       "  width_size=256,\n",
       "  depth=2\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: May 20 2022 19:45:31\n",
      "/home/m_bobrin/anaconda3/envs/icvf/lib/python3.9/site-packages/flax/linen/linear.py:30: DeprecationWarning: jax.ShapedArray is deprecated. Use jax.core.ShapedArray\n",
      "  from jax import ShapedArray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Goal:  (32.33135808427672, 24.222535311056507)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load datafile: 100%|██████████| 8/8 [00:02<00:00,  3.23it/s]\n"
     ]
    }
   ],
   "source": [
    "from icvf_envs.antmaze import d4rl_utils, d4rl_ant, d4rl_pm\n",
    "from src.gc_dataset import GCSDataset\n",
    "\n",
    "gcdataset_config = GCSDataset.get_default_config()\n",
    "\n",
    "env = d4rl_utils.make_env(\"antmaze-large-diverse-v2\")\n",
    "dataset = d4rl_utils.get_dataset(env)\n",
    "gc_dataset = GCSDataset(dataset, **gcdataset_config.to_dict())\n",
    "example_batch = gc_dataset.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m_bobrin/icvf_release/notebooks/../src/gc_dataset.py:32: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  early_termination = np.zeros((n_trajectories, max_path_length), dtype=np.bool)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['actions', 'dones_float', 'masks', 'next_observations', 'observations', 'rewards', 'desired_rewards', 'desired_masks', 'goals', 'desired_goals'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = gc_dataset.sample_trajectories(2)\n",
    "example_batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Each token - (observations, next_observations, dones_float)\n",
    "For ant_maze obs space is vector of size 29, action - 8. Dones - as positional embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VQVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "import flax.linen as nn\n",
    "import einops\n",
    "import jax.numpy as jnp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class VqvaeConfig:\n",
    "    hidden_dim: int = 64\n",
    "    latent_dim: int = 128\n",
    "    \n",
    "    num_tokens: int = 2 # each pair = one token\n",
    "    trajectory_len: int = 1000\n",
    "    L: int = 4 # number of M = T / L vectors\n",
    "    latent_dim: int = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Optional\n",
    "from flax import linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "class ExponentialMovingAverage(nn.Module):\n",
    "  shape: list\n",
    "  dtype: Any = jnp.float32\n",
    "  decay: float = 0.\n",
    "\n",
    "  def setup(self):\n",
    "    shape = self.shape\n",
    "    dtype = self.dtype\n",
    "    self.hidden = self.variable(\"stats\", \"hidden\", lambda: jnp.zeros(shape, dtype=dtype))\n",
    "    self.average = self.variable(\"stats\", \"average\", lambda: jnp.zeros(shape, dtype=dtype)) # how to deal with initialized?\n",
    "    constant = lambda: jnp.zeros(shape, dtype=jnp.int32)\n",
    "    self.counter = self.variable(\"stats\", \"counter\", constant)\n",
    "\n",
    "  def __call__(\n",
    "      self,\n",
    "      value: jnp.ndarray,\n",
    "      update_stats: bool = True,\n",
    "  ) -> jnp.ndarray:\n",
    "\n",
    "    counter = self.counter.value + 1\n",
    "    decay = jax.lax.convert_element_type(self.decay, value.dtype)\n",
    "    one = jnp.ones([], value.dtype)\n",
    "    hidden = self.hidden.value * decay + value * (one - decay)\n",
    "\n",
    "    average = hidden\n",
    "    average /= (one - jnp.power(decay, counter))\n",
    "    if update_stats:\n",
    "      self.counter.value = counter\n",
    "      self.hidden.value = hidden\n",
    "      self.average.value = average\n",
    "    return average\n",
    "\n",
    "# inspired from Haiku's corresponding code to Flax\n",
    "# https://github.com/deepmind/dm-haiku/blob/master/haiku/_src/nets/vqvae.py\n",
    "\n",
    "class VectorQuantizerEMA(nn.Module):\n",
    "  embedding_dim: int\n",
    "  num_embeddings: int\n",
    "  commitment_cost: float\n",
    "  decay: float\n",
    "  epsilon: float = 1e-5\n",
    "  dtype: Any = jnp.float32\n",
    "  cross_replica_axis: Optional[str] = None  \n",
    "  initialized: bool = False\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, inputs, is_training, rng=None, encoding_indices=None):\n",
    "    embedding_shape = [self.embedding_dim, self.num_embeddings]\n",
    "    assert self.dtype == jnp.float32\n",
    "    ema_cluster_size = ExponentialMovingAverage([self.num_embeddings], self.dtype, decay=self.decay)\n",
    "    ema_dw = ExponentialMovingAverage(embedding_shape, self.dtype, decay=self.decay)\n",
    "    initialized = self.has_variable('stats', 'embeddings')\n",
    "    embeddings = self.variable(\"stats\", \"embeddings\", nn.initializers.lecun_uniform(), jax.random.PRNGKey(42), embedding_shape)\n",
    "    \n",
    "    def quantize(encoding_indices):\n",
    "        \"\"\"Returns embedding tensor for a batch of indices.\"\"\"\n",
    "        w = embeddings.value.swapaxes(1, 0)\n",
    "        w = jax.device_put(w)  # Required when embeddings is a NumPy array.\n",
    "        return w[(encoding_indices,)]\n",
    "\n",
    "    if encoding_indices is not None:\n",
    "        return quantize(encoding_indices)\n",
    "    \n",
    "    if not initialized:\n",
    "        hidden, counter, average = ema_cluster_size.hidden, ema_cluster_size.counter, ema_cluster_size.average\n",
    "        hidden, counter, average = ema_dw.hidden, ema_dw.counter, ema_dw.average     \n",
    "        return {\n",
    "            \"quantize\": inputs,\n",
    "            \"loss\": inputs.mean(),\n",
    "        }\n",
    "    \n",
    "    flat_inputs = jnp.reshape(inputs, [-1, self.embedding_dim])\n",
    "    distances = (\n",
    "        jnp.sum(flat_inputs**2, 1, keepdims=True) -\n",
    "        2 * jnp.matmul(flat_inputs, embeddings.value) +\n",
    "        jnp.sum(embeddings.value**2, 0, keepdims=True))\n",
    "\n",
    "    encoding_indices = jnp.argmax(-distances, 1)\n",
    "    encodings = jax.nn.one_hot(encoding_indices,\n",
    "                               self.num_embeddings,\n",
    "                               dtype=distances.dtype)\n",
    "\n",
    "    encoding_indices = jnp.reshape(encoding_indices, inputs.shape[:-1])\n",
    "    quantized = quantize(encoding_indices)\n",
    "    e_latent_loss = jnp.mean((jax.lax.stop_gradient(quantized) - inputs)**2)\n",
    "\n",
    "    if is_training:\n",
    "      cluster_size = jnp.sum(encodings, axis=0)\n",
    "      if self.cross_replica_axis:\n",
    "        cluster_size = jax.lax.psum(\n",
    "            cluster_size, axis_name=self.cross_replica_axis)\n",
    "      updated_ema_cluster_size = ema_cluster_size(cluster_size, update_stats=is_training)\n",
    "\n",
    "      dw = jnp.matmul(flat_inputs.T, encodings)\n",
    "      if self.cross_replica_axis:\n",
    "        dw = jax.lax.psum(dw, axis_name=self.cross_replica_axis)\n",
    "      updated_ema_dw = ema_dw(dw, update_stats=is_training)\n",
    "\n",
    "      n = jnp.sum(updated_ema_cluster_size)\n",
    "      updated_ema_cluster_size = ((updated_ema_cluster_size + self.epsilon) /\n",
    "                                  (n + self.num_embeddings * self.epsilon) * n)\n",
    "\n",
    "      normalised_updated_ema_w = (\n",
    "          updated_ema_dw / jnp.reshape(updated_ema_cluster_size, [1, -1]))\n",
    "\n",
    "      embeddings.value = normalised_updated_ema_w\n",
    "      loss = self.commitment_cost * e_latent_loss\n",
    "\n",
    "    else:\n",
    "      loss = self.commitment_cost * e_latent_loss\n",
    "\n",
    "    # Straight Through Estimator\n",
    "    quantized = inputs + jax.lax.stop_gradient(quantized - inputs)\n",
    "    avg_probs = jnp.mean(encodings, 0)\n",
    "    if self.cross_replica_axis:\n",
    "      avg_probs = jax.lax.pmean(avg_probs, axis_name=self.cross_replica_axis)\n",
    "    perplexity = jnp.exp(-jnp.sum(avg_probs * jnp.log(avg_probs + 1e-10)))\n",
    "\n",
    "    return {\n",
    "        \"quantize\": quantized,\n",
    "        \"loss\": loss,\n",
    "        \"perplexity\": perplexity,\n",
    "        \"encodings\": encodings,\n",
    "        \"encoding_indices\": encoding_indices,\n",
    "        \"distances\": distances,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import jax.numpy as jnp\n",
    "from einops import rearrange, repeat\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from jaxrl_m.networks import MLP\n",
    "from jax._src import prng\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    hyperparams: VqvaeConfig\n",
    "    vq_class: VectorQuantizerEMA\n",
    "    mlp: MLP\n",
    "    quant_key: prng.PRNGKeyArray\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, inputs, state) -> Any: #state\n",
    "        # B x T x embedding\n",
    "        B, T, observation_dim = inputs.shape\n",
    "        token_emb = self.mlp(hidden_dims=(self.hyperparams.hidden_dim, self.hyperparams.latent_dim))(inputs)\n",
    "        #token_emb = nn.Dense(features=self.hyperparams.latent_dim)(token_emb)\n",
    "        \n",
    "        pos_embedding = self.param('pos_embedding', nn.initializers.zeros, [B, self.hyperparams.trajectory_len+1, self.hyperparams.latent_dim])\n",
    "        goal_token = self.param('goal', nn.initializers.zeros, [B, 1, self.hyperparams.latent_dim])\n",
    "        token_emb = jnp.concatenate([token_emb, goal_token], axis=1)\n",
    "        token_emb += pos_embedding[:self.hyperparams.trajectory_len + 1]\n",
    "        \n",
    "        #B x (T // L) x embedding_dim\n",
    "        # like in TAP paper MaxPool with window = 3\n",
    "        token_emb = nn.max_pool(token_emb, window_shape=(self.hyperparams.L, ), strides=(self.hyperparams.L, ))\n",
    "        token_emb = self.mlp(hidden_dims=(self.hyperparams.latent_dim, ))(token_emb)\n",
    "        vq_info = self.quantize(token_emb)\n",
    "        \n",
    "        # Decoding stage\n",
    "        # B x (T // Latent) x embedding_dim\n",
    "        # State: B x observ_dim\n",
    "        latents = vq_info['quantize']\n",
    "        # predict full trajectory\n",
    "        decoded_traj = self.decode(latents, state, observation_dim)\n",
    "        print(decoded_traj.shape)\n",
    "        return vq_info, decoded_traj\n",
    "    \n",
    "    def decode(self, latents, state, observation_dim):\n",
    "        B, T, _ = latents.shape\n",
    "        \n",
    "        state_flat = einops.repeat(einops.rearrange(state, 'B E -> B 1 E'), 'B 1 E -> B T E', T=T)\n",
    "        inputs = jnp.concatenate([state_flat, latents], axis=-1)\n",
    "        inputs = jnp.repeat(inputs, repeats=self.hyperparams.L, axis=1)\n",
    "        inputs = self.mlp(hidden_dims=(observation_dim, observation_dim))(inputs)\n",
    "        return inputs\n",
    "        \n",
    "    def quantize(self, token_emb):\n",
    "        vq_vars = self.vq_class.init({'params':self.quant_key}, token_emb, is_training=True)\n",
    "        vq_info = self.vq_class.apply(vq_vars, token_emb, is_training=False)\n",
    "        \n",
    "        return vq_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1000, 58)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0][None, ...].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1000, 58)\n"
     ]
    }
   ],
   "source": [
    "base_key = jax.random.PRNGKey(42)\n",
    "model_key, quantization_key, dummy_states = jax.random.split(base_key, 3)\n",
    "\n",
    "# B x Tokens x observ_dim\n",
    "vq_config = VqvaeConfig()\n",
    "vq_class = VectorQuantizerEMA(embedding_dim=512, num_embeddings=128, commitment_cost=0.2, decay=0.1)\n",
    "\n",
    "model = VQVAE(vq_config, vq_class, MLP, quantization_key)\n",
    "dummy_states = jax.random.uniform(dummy_states, (1, 32))\n",
    "\n",
    "init_vars = model.init(model_key, test[0][None, ...], dummy_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1000, 58)\n"
     ]
    }
   ],
   "source": [
    "vq_info, decoded_traj = model.apply(init_vars, test[0][None, ...], dummy_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
