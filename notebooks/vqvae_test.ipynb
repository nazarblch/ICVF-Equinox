{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import jax\n",
    "import os\n",
    "os.environ[\"D4RL_SUPPRESS_IMPORT_ERROR\"] = \"1\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: May 20 2022 19:45:31\n",
      "/home/m_bobrin/anaconda3/envs/icvf/lib/python3.9/site-packages/flax/linen/linear.py:30: DeprecationWarning: jax.ShapedArray is deprecated. Use jax.core.ShapedArray\n",
      "  from jax import ShapedArray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Goal:  (33.14731094862936, 24.45574920803667)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load datafile: 100%|██████████| 8/8 [00:02<00:00,  3.23it/s]\n"
     ]
    }
   ],
   "source": [
    "from icvf_envs.antmaze import d4rl_utils, d4rl_ant, d4rl_pm\n",
    "from src.gc_dataset import GCSDataset\n",
    "\n",
    "gcdataset_config = GCSDataset.get_default_config()\n",
    "\n",
    "env = d4rl_utils.make_env(\"antmaze-large-diverse-v2\")\n",
    "dataset = d4rl_utils.get_dataset(env)\n",
    "gc_dataset = GCSDataset(dataset, **gcdataset_config.to_dict())\n",
    "example_batch = gc_dataset.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m_bobrin/icvf_release/notebooks/../src/gc_dataset.py:32: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  early_termination = np.zeros((n_trajectories, max_path_length), dtype=np.bool)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['actions', 'dones_float', 'masks', 'next_observations', 'observations', 'rewards', 'desired_rewards', 'desired_masks', 'goals', 'desired_goals'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = gc_dataset.sample_trajectories(2)\n",
    "example_batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Each token - (observations, next_observations, dones_float)\n",
    "For ant_maze obs space is vector of size 29, action - 8. Dones - as positional embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VQVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "import flax.linen as nn\n",
    "import einops\n",
    "import jax.numpy as jnp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class VqvaeConfig:\n",
    "    embedding_dim: int = 128\n",
    "    num_tokens: int = 2 # each pair = one token\n",
    "    trajectory_len: int = 1000\n",
    "    L: int = 3 # number of M = T / L vectors\n",
    "    latent_dim: int = 512\n",
    "    \n",
    "    run_opt: str = field(default='train')\n",
    "    adam_beta1: float = .9\n",
    "    adam_beta2: float = .9\n",
    "    lr: float = 3e-5\n",
    "    ema_rate: float = 0.\n",
    "    n_batch: int = 32   \n",
    "    warmup_iters: float = 100.\n",
    "    wd: float = 0.\n",
    "    grad_clip: float = 200.     \n",
    "    dtype: str = \"float32\"\n",
    "    checkpoint: bool = False\n",
    "    \n",
    "    iters_per_ckpt: int = 25000\n",
    "    iters_per_images: int = 10000\n",
    "    iters_per_print: int = 1000\n",
    "    iters_per_save: int = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Optional\n",
    "from flax import linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "class ExponentialMovingAverage(nn.Module):\n",
    "  shape: list\n",
    "  dtype: Any = jnp.float32\n",
    "  decay: float = 0.\n",
    "\n",
    "  def setup(self):\n",
    "    shape = self.shape\n",
    "    dtype = self.dtype\n",
    "    self.hidden = self.variable(\"stats\", \"hidden\", lambda: jnp.zeros(shape, dtype=dtype))\n",
    "    self.average = self.variable(\"stats\", \"average\", lambda: jnp.zeros(shape, dtype=dtype)) # how to deal with initialized?\n",
    "    constant = lambda: jnp.zeros(shape, dtype=jnp.int32)\n",
    "    self.counter = self.variable(\"stats\", \"counter\", constant)\n",
    "\n",
    "  def __call__(\n",
    "      self,\n",
    "      value: jnp.ndarray,\n",
    "      update_stats: bool = True,\n",
    "  ) -> jnp.ndarray:\n",
    "\n",
    "    counter = self.counter.value + 1\n",
    "    decay = jax.lax.convert_element_type(self.decay, value.dtype)\n",
    "    one = jnp.ones([], value.dtype)\n",
    "    hidden = self.hidden.value * decay + value * (one - decay)\n",
    "\n",
    "    average = hidden\n",
    "    average /= (one - jnp.power(decay, counter))\n",
    "    if update_stats:\n",
    "      self.counter.value = counter\n",
    "      self.hidden.value = hidden\n",
    "      self.average.value = average\n",
    "    return average\n",
    "\n",
    "# inspired from Haiku's corresponding code to Flax\n",
    "# https://github.com/deepmind/dm-haiku/blob/master/haiku/_src/nets/vqvae.py\n",
    "\n",
    "class VectorQuantizerEMA(nn.Module):\n",
    "  embedding_dim: int\n",
    "  num_embeddings: int\n",
    "  commitment_cost: float\n",
    "  decay: float\n",
    "  epsilon: float = 1e-5\n",
    "  dtype: Any = jnp.float32\n",
    "  cross_replica_axis: Optional[str] = None  \n",
    "  initialized: bool = False\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, inputs, is_training, rng=None, encoding_indices=None):\n",
    "    embedding_shape = [self.embedding_dim, self.num_embeddings]\n",
    "    assert self.dtype == jnp.float32\n",
    "    ema_cluster_size = ExponentialMovingAverage([self.num_embeddings], self.dtype, decay=self.decay)\n",
    "    ema_dw = ExponentialMovingAverage(embedding_shape, self.dtype, decay=self.decay)\n",
    "    initialized = self.has_variable('stats', 'embeddings')\n",
    "    embeddings = self.variable(\"stats\", \"embeddings\", nn.initializers.lecun_uniform(), rng, embedding_shape)\n",
    "    \n",
    "    def quantize(encoding_indices):\n",
    "        \"\"\"Returns embedding tensor for a batch of indices.\"\"\"\n",
    "        w = embeddings.value.swapaxes(1, 0)\n",
    "        w = jax.device_put(w)  # Required when embeddings is a NumPy array.\n",
    "        return w[(encoding_indices,)]\n",
    "\n",
    "    if encoding_indices is not None:\n",
    "        return quantize(encoding_indices)\n",
    "    \n",
    "    if not initialized:\n",
    "        hidden, counter, average = ema_cluster_size.hidden, ema_cluster_size.counter, ema_cluster_size.average\n",
    "        hidden, counter, average = ema_dw.hidden, ema_dw.counter, ema_dw.average     \n",
    "        return {\n",
    "            \"quantize\": inputs,\n",
    "            \"loss\": inputs.mean(),\n",
    "        }\n",
    "    \n",
    "    flat_inputs = jnp.reshape(inputs, [-1, self.embedding_dim])\n",
    "    distances = (\n",
    "        jnp.sum(flat_inputs**2, 1, keepdims=True) -\n",
    "        2 * jnp.matmul(flat_inputs, embeddings.value) +\n",
    "        jnp.sum(embeddings.value**2, 0, keepdims=True))\n",
    "\n",
    "    encoding_indices = jnp.argmax(-distances, 1)\n",
    "    encodings = jax.nn.one_hot(encoding_indices,\n",
    "                               self.num_embeddings,\n",
    "                               dtype=distances.dtype)\n",
    "\n",
    "    encoding_indices = jnp.reshape(encoding_indices, inputs.shape[:-1])\n",
    "    quantized = quantize(encoding_indices)\n",
    "    e_latent_loss = jnp.mean((jax.lax.stop_gradient(quantized) - inputs)**2)\n",
    "\n",
    "    if is_training:\n",
    "      cluster_size = jnp.sum(encodings, axis=0)\n",
    "      if self.cross_replica_axis:\n",
    "        cluster_size = jax.lax.psum(\n",
    "            cluster_size, axis_name=self.cross_replica_axis)\n",
    "      updated_ema_cluster_size = ema_cluster_size(cluster_size, update_stats=is_training)\n",
    "\n",
    "      dw = jnp.matmul(flat_inputs.T, encodings)\n",
    "      if self.cross_replica_axis:\n",
    "        dw = jax.lax.psum(dw, axis_name=self.cross_replica_axis)\n",
    "      updated_ema_dw = ema_dw(dw, update_stats=is_training)\n",
    "\n",
    "      n = jnp.sum(updated_ema_cluster_size)\n",
    "      updated_ema_cluster_size = ((updated_ema_cluster_size + self.epsilon) /\n",
    "                                  (n + self.num_embeddings * self.epsilon) * n)\n",
    "\n",
    "      normalised_updated_ema_w = (\n",
    "          updated_ema_dw / jnp.reshape(updated_ema_cluster_size, [1, -1]))\n",
    "\n",
    "      embeddings.value = normalised_updated_ema_w\n",
    "      loss = self.commitment_cost * e_latent_loss\n",
    "\n",
    "    else:\n",
    "      loss = self.commitment_cost * e_latent_loss\n",
    "\n",
    "    # Straight Through Estimator\n",
    "    quantized = inputs + jax.lax.stop_gradient(quantized - inputs)\n",
    "    avg_probs = jnp.mean(encodings, 0)\n",
    "    if self.cross_replica_axis:\n",
    "      avg_probs = jax.lax.pmean(avg_probs, axis_name=self.cross_replica_axis)\n",
    "    perplexity = jnp.exp(-jnp.sum(avg_probs * jnp.log(avg_probs + 1e-10)))\n",
    "\n",
    "    return {\n",
    "        \"quantize\": quantized,\n",
    "        \"loss\": loss,\n",
    "        \"perplexity\": perplexity,\n",
    "        \"encodings\": encodings,\n",
    "        \"encoding_indices\": encoding_indices,\n",
    "        \"distances\": distances,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import jax.numpy as jnp\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    hyperparams: VqvaeConfig\n",
    "    vq_class: VectorQuantizerEMA\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, inputs) -> Any: #state\n",
    "        # B x T x embedding\n",
    "        B, T, emd = inputs.shape\n",
    "        token_emb = nn.Dense(features=self.hyperparams.embedding_dim)(inputs)\n",
    "        token_emb = nn.Dense(features=self.hyperparams.latent_dim)(token_emb)\n",
    "        \n",
    "        pos_embedding = self.param('pos_embedding', nn.initializers.zeros, [B, self.hyperparams.trajectory_len+1, self.hyperparams.latent_dim])\n",
    "        goal_token = self.param('goal', nn.initializers.zeros, [B, 1, self.hyperparams.latent_dim])\n",
    "        token_emb = jnp.concatenate([token_emb, goal_token], axis=1)\n",
    "        token_emb += pos_embedding[:self.hyperparams.trajectory_len + 1]\n",
    "        #B x (T // L) x embedding_dim\n",
    "        token_emb = nn.max_pool(token_emb, window_shape=(self.hyperparams.L, ), strides=(self.hyperparams.L, ))\n",
    "        token_emb = nn.Dense(features=self.hyperparams.latent_dim)(token_emb)\n",
    "        \n",
    "        vq_info = self.quantize(token_emb)\n",
    "        # vq_vars = self.vq.init(jax.random.PRNGKey(0), token_emb, is_training=True, rng=jax.random.PRNGKey(42))\n",
    "        # vq_info = self.vq.apply(vq_vars, token_emb, is_training=False)\n",
    "        \n",
    "        return vq_info\n",
    "    \n",
    "    def quantize(self, token_emb):\n",
    "        vq_vars = self.vq_class.init(jax.random.PRNGKey(0), token_emb, is_training=True, rng=jax.random.PRNGKey(42))\n",
    "        vq_info = self.vq_class.apply(vq_vars, token_emb, is_training=False)\n",
    "        \n",
    "        return vq_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "key1, keys = jax.random.split(key, 2)\n",
    "\n",
    "# B x Tokens x observ_dim\n",
    "vq = VectorQuantizerEMA(embedding_dim=512, num_embeddings=128, commitment_cost=0.2, decay=0.1)\n",
    "model = VQVAE(VqvaeConfig, vq)\n",
    "init_vars = model.init(key1, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.apply(init_vars, test)['perplexity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icvf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
